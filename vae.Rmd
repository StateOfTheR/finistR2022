---
title: "VAE"
author: "finister2022"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(tidyverse)
library(torch)
```

## Simulation de données

Ce document illustre un essai d'auto-encodeur variationnel utilisant la librairie `torch`.

On commence par simuler des données suivant une gaussienne pseudo-dégénérée, via une matrice $A$ de taille $15 \times 3$

$$
\begin{align*}
Z & \sim \mathcal{N}_3(0, I_3) \\
Y = AZ + E & \sim \mathcal{N}_{15}(0, AA^T + \sigma^2 I_{15})
\end{align*}
$$

```{r}
set.seed(42)
n <- 200
k <- 3
p <- 15
sigma <- 0.1
A <- matrix(rnorm(p*k), nrow = p, ncol = k)
Z <- matrix(rnorm(n*k), nrow = n, ncol = k)
Y <- Z %*% t(A) + sigma * matrix(rnorm(n*p), nrow = n, ncol = p)
input <- torch_tensor(Y)
```

On va construire un auto-encoder avec un espace latent de dimension `latent_dim` égale à 2 pour pouvoir faire des dessins.

```{r}
latent_dim <- 2
```

## Définition d'un encodeur

On définit ensuite un encodeur comme un réseau très simple avec deux couches linéaires de tailles 10 puis 5 et des fonctions d'activation relu (hormis pour la couche finale de répresentation). 

La construction passe par la définition d'un module (via `nn_module()`) auquel il faut (au moins) fournir 2 méthodes:

- `initialize()` qui indique comment initialiser une nouvelle instance du réseau (et définit l'architecture du réseau)
- `forward()` qui indique comment réaliser les calculs

```{r}
encoder <- nn_module(
  classname = "encoder", 
  ## Définition des couches
  initialize = function(in_features, latent_dim) {
    self$linear1 <- nn_linear(in_features, 10)
    self$linear2 <- nn_linear(10, 5)
    self$mean <- nn_linear(5, latent_dim)
    self$log_var  <- nn_linear(5, latent_dim)
  }, 
  ## Définitions des calculs
  forward = function(input) {
    ## Combinaison linéaire des features dans la première couche
    input <- self$linear1(input)
    ## Activation relu
    input <- nnf_relu(input)
    ## Combinaison linéaire des features dans la deuxième couche
    input <- self$linear2(input)
    ## Activation relu
    input <- nnf_relu(input)
    ## Création des paramètres de moyenne et de variance
    mean <- self$mean(input)
    log_var   <- self$log_var(input)
    ## L'encodeur renvoie mean et sd
    list(mean    = mean, 
         log_var = log_var)
  }
)
```

On peut aussi définir ce module de façon plus compacte en utilisant `nn_sequential()` pour chaîner des module et indiquer dans la définition les fonctions d'activation à utiliser. 

```{r}
## Création d'un module compressor
create_compressor <- function(in_features) {
  nn_sequential(
    nn_linear(in_features, 10),
    nn_relu(),
    nn_linear(10, 5),
    nn_relu()
  )  
}
## Création de l'encodeur à l'aide du compresseur
encoder <- nn_module(
  classname = "encoder", 
  ## Définition des couches
  initialize = function(in_features, latent_dim) {
    self$compressor <- create_compressor(in_features)
    self$mean <- nn_linear(5, latent_dim)
    self$log_var  <- nn_linear(5, latent_dim)
  }, 
  ## Définitions des calculs
  forward = function(input) {
    ## Calcul des répresentations compressées
    compressed <- self$compressor(input)
    ## Création des paramètres de moyenne et de variance
    mean <- self$mean(compressed)
    log_var   <- self$log_var(compressed)
    ## L'encodeur renvoie mean et log_var
    list(mean    = mean, 
         log_var = log_var)
  }
)
```

L'encodeur produit deux vecteurs de taille `latent_dim` à partir d'un vecteur de données (de taille 15). 

On peut le vérifier sur un exemple simple

```{r}
enc <- encoder(15, 2)
enc(input[1:5, ])
```

## Définition d'un décodeur

On crée ensuite notre décodeur de la même façon. Ce dernier va partir d'un vecteur de taille `latent_dim` pour construire un vecteur de taille 15. Par souci de simplicité, on adopte une architecture symétrique à celle du décodeur à l'exception de la dernière couche (purement linéaire). 

```{r}
## Création d'un module decompressor
create_decompressor <- function(latent_dim, out_features) {
  nn_sequential(
    nn_linear(latent_dim, 5),
    nn_relu(),
    nn_linear(5, 10),
    nn_relu(),
    nn_linear(10, out_features),
  )  
}
## Création du decodeur à l'aide du decompresseur
decoder <- nn_module(
  classname = "decoder", 
  ## Définition des couches
  initialize = function(latent_dim, out_features) {
    self$decompressor <- create_decompressor(latent_dim, out_features)
  }, 
  ## Définitions des calculs
  forward = function(input) {
    self$decompressor(input)
  }
)
```

Le décodeur produit un vecteur de taille `out_features` à partir d'un vecteur de taille `latent_dim`. On peut le vérifier sur un exemple simple. 

```{r}
dec <- decoder(2, 15)
latent_vectors <- matrix(0, nrow = 5, ncol = 2) |> torch_tensor() 
dec(latent_vectors)
```

## Définition du VAE

La dernière étape consiste à coupler l'encodeur et le décodeur via un échantillonneur:

```{r}
vae_module <- nn_module(
  classname = "sampler", 
  initialize = function(n_features, latent_dim) {
    self$latent_dim <- latent_dim
    self$encoder <- encoder(n_features, latent_dim)
    self$decoder <- decoder(latent_dim, n_features)
  },
  forward = function(input) {
    ## compression des données
    comp_input <- self$encoder(input)
    mean <- comp_input$mean
    log_var <- comp_input$log_var
    ## échantillonnage dans l'espace latent
    z <- mean + torch_exp(log_var$mul(0.5))*torch_randn(c(dim(input)[1], self$latent_dim))
    ## décompression de la représentation latente
    decomp_input <- self$decoder(z)
    
    list(decomp_input = decomp_input, 
         z            = z, 
         mean         = mean, 
         log_var      = log_var)
  }
)
```

Pour chaque vecteur de taille `n_features`, notre vae produit:

- 3 vecteurs de taille `latent_dim` (dont un aléatoire)
- 1 vecteurs de taille `n_features`

```{r}
vae <- vae_module(n_features = 15, latent_dim = 2)
vae(input[1, , drop = FALSE])
```

On peut vérifier que le composant `z` est aléatoire en appelant `vae()` deux fois. 

```{r}
## Premier appel
vae(input[1, , drop = FALSE])$z
## Deuxième appel
vae(input[1, , drop = FALSE])$z
```

## Création du VAE et de l'optimiseur

On (ré-)initialise notre VAE

```{r}
vae <- vae_module(n_features = 15, latent_dim = 2)
```

Et on prépare un optimiseur

```{r}
optimizer_vae <- optim_adam(vae$parameters, lr = 0.001)
```

## Entraînement du VAE

Dans cette partie, on se contente de charger l'ensemble des données, on ne travaille par batch (ce qui nécessiterait de définir un `dataloader()`). 

On commence par définir une fonction de perte comme la somme de l'entropie croisée et de la divergence de Kullback-Leibler

```{r}
loss_fn <- function(prediction, target, mean, log_var) {
  ## Perte L2 pour la reconstruction 
  cross_entropy <- nn_mse_loss(reduction = "sum")
  
  ## KL part of the loss
  kl <- function(mean, log_var) {
    kl_div <- 1 + log_var - mean$square() - log_var$exp()
    kl_div$multiply(-0.5)$sum()
  }
  
  ## Addition des deux 
  cross_entropy(prediction, target) + kl(mean, log_var)  
}
eval_loss <- function(input) {
  target <- input
  
  ## Extraction des composant prediction, mean, log_var depuis le VAE
  results <- vae(input)
  mean <- results$mean
  log_var <- results$log_var
  prediction <- results$decomp_input
  
  loss_fn(prediction, target, mean, log_var)
}
```

On vérifie sur un exemple simple que la fonction de perte est bien définie et vaut ce qu'on pense sur des exemples simples. 

```{r}
loss_fn(prediction = input[1, , drop = FALSE], 
        target = input[1, , drop = FALSE], 
        mean = torch_zeros(2),
        log_var = torch_zeros(2))
```

Puis on entraîne notre modèle (comme vu dans l'atelier sur `torch`) pendant 100 itérations. 
```{r}
num_iterations <- 100
loss_vector <- vector("numeric", num_iterations)
for (i in 1:num_iterations) {
  optimizer_vae$zero_grad()
  loss <- eval_loss(input)
  loss$backward()
  loss_vector[i] <- loss |> as.numeric()
  optimizer_vae$step()
}
```

On peut regarder l'évolution de la perte au fil du temps. 

```{r}
plot(1:num_iterations, loss_vector)
```

 ## Exploration des résultats

On peut visualiser la représentation des données dans notre espace latent
 
```{r}
latents <- vae(input)$mean
```
 
Et les afficher

```{r, fig.height=6, fig.width=7}
latents |> as.matrix() |> `colnames<-`(c("x", "y")) |> as_tibble() |> 
  ggplot() + aes(x, y) + 
  geom_point() + 
  coord_equal() + 
  theme_minimal() + 
  theme(text = element_text(size = 16))
```

On peut aussi vérifier comment se positionne les reconstructions par rapport aux données originales (après avoir fait une ACP sur le tableau complet). 
 
```{r}
reconstructions <- vae(input)$decomp_input |> 
  as.matrix() %>% `colnames<-`(paste0("D", 1:ncol(.)))
pca <- prcomp(rbind(Y, reconstructions), scale. = TRUE, center = TRUE)
point_data <- pca$x %>% as_tibble() %>% mutate(origin = rep(c("true", "fake"), each = nrow(Y)))
line_data <- point_data |> mutate(ID = rep(1:nrow(Y), 2)) |> 
  pivot_wider(id_cols = ID, names_from = origin, values_from = starts_with("PC"))
```
 
```{r, fig.height=5, fig.width=7}
ggplot() + 
  geom_point(data = point_data, mapping = aes(x = PC1, y = PC2, color = origin)) + 
  geom_segment(data = line_data, mapping = aes(x = PC1_true, xend = PC1_fake, y = PC2_true, yend = PC2_fake)) + 
  coord_equal() + 
  theme_minimal() + 
  theme(text = element_text(size = 16))
```

 