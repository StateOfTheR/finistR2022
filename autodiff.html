<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Félix Cheysson, Julien Chiquet, Mahendra Mariadassou, Tristan Mary-Huard" />

<meta name="date" content="2022-09-29" />

<title>autodiff</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>







<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Finist'R 2022</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="https://github.com/StateOfTheR/finistR2022">
    <span class="fa fa-github"></span>
     
    
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Workflow
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="introDockeR.html">Docker</a>
    </li>
    <li>
      <a href="rebase.html">Git avancé</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Développement
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="pipe.html">pipe</a>
    </li>
    <li>
      <a href="future.html">future</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fas fa-chart-bar"></span>
     
    Analyse de données et modélisation
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="webscraping.html">Webscrapping</a>
    </li>
    <li>
      <a href="sig_with_sf.html">R et SIG</a>
    </li>
    <li class="dropdown-header">R et Bayésien</li>
    <li class="dropdown-header">tidymodels</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fas fa-chart-network"></span>
     
    Visualisation et interaction
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="extensionShiny.html">Shiny extension</a>
    </li>
    <li>
      <a href="graphViz.html">Visualisation de réseaux</a>
    </li>
    <li>
      <a href="addins.html">Addins</a>
    </li>
  </ul>
</li>
<li>
  <a href="QuartoTest.html">
    <span class="fa fa-microphone"></span>
     
    Quarto
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Machine learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="autodiff.html">Différentiation automatique</a>
    </li>
    <li>
      <a href="vae.html">Autoencodeurs variationnels</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">autodiff</h1>
<h4 class="author">Félix Cheysson, Julien Chiquet, Mahendra Mariadassou,
Tristan Mary-Huard</h4>
<h4 class="date">2022-09-29</h4>

</div>


<div id="exploration-de-torch-pour-la-différentiation-automatique"
class="section level1">
<h1>Exploration de <code>{{torch}}</code> pour la différentiation
automatique</h1>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>Le package <a href=""><code>torch</code></a> permet de faire de la
différentiation automatique à condition de réécrire son code avec les
fonctions de <code>torch</code>. Il est basé sur la librairie C++
<code>libtorch</code> fournie dans PyTorch. Contrairement à d’anciennes
versions, torch ne <strong>fait pas</strong> appel à python et ne
nécessite pas reticulate. L’installation en est d’autant plus
simple:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(torch)</span></code></pre></div>
<p>Lors de la première utilisation, <code>torch</code> vous demandera de
télécharger des fichiers supplémentaires via:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>torch<span class="sc">::</span><span class="fu">install_torch</span>() <span class="do">## si vous avez un GPU compatible avec CUDA</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">## torch::install_torch(type = &quot;cpu&quot;) ## sinon</span></span></code></pre></div>
<p><code>torch</code> est surtout utilisé pour des applications en ML/IA
mais on peut aussi l’utiliser pour des calculs de gradients, hessiennes
et de l’optimisation dans des modèles plus simples. On va l’illustrer
ici pour de la régression logistique.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.0      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span></code></pre></div>
</div>
<div id="principe-du-calcul-de-gradient" class="section level2">
<h2>Principe du calcul de gradient</h2>
<p><code>torch</code> fonctionne avec ses propres types numériques,
qu’il faut créer avec la fonction <code>torch_tensor()</code> et ses
propres fonctions <code>torch_*()</code>. Considérons un exemple très
simple: $ x x^2$</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">3</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">torch_square</span>(x)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x; y</span></code></pre></div>
<pre><code>## torch_tensor
##  3
## [ CPUFloatType{1} ]</code></pre>
<pre><code>## torch_tensor
##  9
## [ CPUFloatType{1} ]</code></pre>
<p>On va pouvoir calculer <span
class="math inline">\(\frac{dy}{dx}\)</span> en définissant
<code>x</code> avec l’argument <code>require_grad = TRUE</code>. Cet
argument va spécifier que ‘x’ est entrainable et va démarrer
l’enregistrement par autograd des opérations sur ce tenseur.</p>
<p>Autograd est un module de <code>torch</code> qui permet de collecter
les gradients. Il le fait en enregistrant des données (tenseurs) et
toutes les opérations exécutées dans un graphe acyclique dirigé dont les
feuilles sont les tenseurs d’entrée et les racines les tenseurs de
sorties. Ces opérations sont stockées comme des fonctions et au moment
du calcul des gradients, sont appliquées depuis le noeud de sortie en
‘backpropagation’ le long du réseau.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<pre><code>## torch_tensor
##  2
## [ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
<p>On remarque que <code>x</code> possède désormais un champ
<code>$grad</code> (même si ce dernier n’est pas encore défini).</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<p>Lorsqu’on calcule <span class="math inline">\(y = x^2\)</span>, ce
dernier va également hériter d’un nouveau champ
<code>$grad_fn</code>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">torch_log</span>(<span class="fu">torch_square</span>(x))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>y</span></code></pre></div>
<pre><code>## torch_tensor
##  1.3863
## [ CPUFloatType{1} ][ grad_fn = &lt;LogBackward0&gt; ]</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y<span class="sc">$</span>grad_fn</span></code></pre></div>
<pre><code>## LogBackward0</code></pre>
<p>qui indique comment calculer le gradient en utilisant la dérivée des
fonctions composées:</p>
<p><span class="math display">\[
(g\circ f)&#39;(x) = f&#39;(x) \times g&#39;(f(x))
\]</span></p>
<p>et les fonctions</p>
<p><span class="math display">\[
\frac{dx^2}{dx} = 2x \quad \frac{d \log(x)}{dx} = \frac{1}{x}
\]</span></p>
<p>Le calcul effectif du gradient est déclenché lors de l’appel à la
méthode <code>$backward()</code> de <code>y</code> et est stocké dans le
champ <code>$grad</code> de <code>x</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x<span class="sc">$</span>grad <span class="do">## gradient non défini</span></span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>y<span class="sc">$</span><span class="fu">backward</span>() </span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x<span class="sc">$</span>grad <span class="do">## gradient défini = 1</span></span></code></pre></div>
<pre><code>## torch_tensor
##  1
## [ CPUFloatType{1} ]</code></pre>
<p>On a bien:</p>
<p><span class="math display">\[
\frac{dy}{dx} = \underbrace{\frac{dy}{dz}}_{\log}(z) \times
\underbrace{\frac{dz}{dx}}_{\text{power}}(x) = \frac{1}{4} \times 2*2 =
1
\]</span> Intuitivement au moment du calcul de <code>y</code>,
<code>torch</code> construit un graphe computationnel qui lui permet
d’évaluer <strong>numériquement</strong> <span
class="math inline">\(y\)</span> et qui va <strong>également</strong>
servir pour calculer <span class="math inline">\(\frac{dy}{dz}\)</span>
au moment de l’appel à la fonction <code>$backward()</code> issue du
module autograd.</p>
<p>Essayons de reproduire le calcul dans notre exemple. Le calcul
<strong>forward</strong> donne</p>
<p><span class="math display">\[
x = 2 \xrightarrow{x \mapsto x^2} z = 4 \mapsto \xrightarrow{x \mapsto
\log(x)} y = \log(4)
\]</span></p>
<p>Pour le calcul <strong>backward</strong>, il faut donc construire le
graphe formel suivant. La première étape du graphe est accessible via
<code>$grad_fn</code></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>y<span class="sc">$</span>grad_fn</span></code></pre></div>
<pre><code>## LogBackward0</code></pre>
<p>et les fonctions suivantes via <code>$next_functions</code></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>y<span class="sc">$</span>grad_fn<span class="sc">$</span>next_functions</span></code></pre></div>
<pre><code>## [[1]]
## PowBackward0</code></pre>
<p>Dans notre exemple, on a donc:</p>
<p><span class="math display">\[
\frac{dy}{dy} = 1 \xrightarrow{x \mapsto \text{logBackward}(x)}
\frac{dy}{dz} = \frac{dy}{dy} \times \text{logBackward}(z)
\xrightarrow{x \mapsto \text{powerBackward}(x)} \frac{dy}{dx} =
\frac{dy}{dz} \times \text{logBackward}(x)
\]</span></p>
<p>Dans cet exemple:</p>
<ul>
<li><span class="math inline">\(\text{logBackward}(x) =
\frac{1}{x}\)</span></li>
<li><span class="math inline">\(\text{powBackward}(x) = 2x\)</span></li>
</ul>
<p>Et la propagation des dérivées donne donc</p>
<p><span class="math display">\[
\frac{dy}{dy} = 1 \to \frac{dy}{dz} = 1 \times \frac{1}{4} = \frac{1}{4}
\to \frac{dy}{dx} = \frac{1}{4} \times 4 = 1
\]</span></p>
<p>Ce graphe est illustré ci-dessous pour la fonction <span
class="math inline">\((x_1, x_2) \mapsto z = sin(x_2) log(x_1
x_2)\)</span></p>
<p><img
src="https://pytorch.org/assets/images/augmented_computational_graph.png" /></p>
<p>Pour (beaucoup) plus de détails sur le graphe computationnel, on peut
consulter la <a
href="https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/">documentation
officielle de PyTorch</a>.</p>
<p>Il faut juste noter que dans <code>torch</code>, le graphe
computationnel est construit de <strong>façon dynamique</strong>, au
moment du calcul de <code>y</code>.</p>
</div>
<div id="régression-logistique-avec-torch" class="section level2">
<h2>Régression logistique avec <code>torch</code></h2>
<p>On va adopter un simple modèle de régression logistique:</p>
<p><span class="math display">\[
Y_i \sim \mathcal{B}(\sigma(\theta^T x_i)) \quad \text{avec} \quad
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>Le but est d’estimer <span class="math inline">\(\theta\)</span> et
éventuellement les erreurs associées. On commence par générer des
données.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">45</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> n<span class="sc">*</span>p), <span class="at">ncol =</span> p, <span class="at">nrow =</span> n)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">3</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> (X <span class="sc">%*%</span> theta) <span class="sc">%&gt;%</span> <span class="fu">as.vector</span>()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rbernoulli</span>(<span class="at">n =</span> n, <span class="at">p =</span> probs) <span class="sc">+</span> <span class="fl">0.</span></span></code></pre></div>
<p><code>torch</code> fonctionne avec ses propres types numériques,
qu’il faut créer avec la fonction <code>torch_tensor()</code>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(Y)</span></code></pre></div>
<p>On écrit ensuite la fonction de vraisemblance</p>
<p><span class="math display">\[
\mathcal{L}(\mathbf{X}, \mathbf{y}; \theta) = \sum_{i=1}^n y_i
(\theta^Tx_i) - \sum_{i=1}^n log(1 + e^{\theta^T x_i})
\]</span></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>logistic_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, x, y) {</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is</span>(theta, <span class="st">&quot;torch_tensor&quot;</span>)) {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">&quot;theta must be a torch tensor&quot;</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  odds <span class="ot">&lt;-</span> <span class="fu">torch_matmul</span>(x, theta)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  log_lik <span class="ot">&lt;-</span> <span class="fu">torch_dot</span>(y, odds) <span class="sc">-</span> <span class="fu">torch_sum</span>(<span class="fu">torch_log</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">torch_exp</span>(odds)))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>log_lik)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>avant de vérifier qu’elle fonctionne:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logistic_loss</span>(<span class="at">theta =</span> <span class="fu">torch_tensor</span>(theta), <span class="at">x =</span> x, <span class="at">y =</span> y)</span></code></pre></div>
<pre><code>## torch_tensor
## 50.4573
## [ CPUFloatType{} ]</code></pre>
<p>On veut ensuite définir une fonction objective à maximiser (qui ne
dépend que de <code>theta</code>):</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>eval_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, <span class="at">verbose =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">&lt;-</span> <span class="fu">logistic_loss</span>(theta, x, y)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (verbose) {</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cat</span>(<span class="fu">paste</span>(theta <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>(), <span class="at">collapse=</span><span class="st">&quot;, &quot;</span>), <span class="st">&quot;: &quot;</span>, <span class="fu">as.numeric</span>(loss), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(loss)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>et vérifier qu’elle fonctionne</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_loss</span>(<span class="fu">torch_tensor</span>(theta), <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## torch_tensor
## 50.4573
## [ CPUFloatType{} ]</code></pre>
<p>avant de procéder à l’optimisation à proprement parler. Pour cette
dernière, on commence par définir notre paramètre sous forme d’un
tenseur qui va être mis à jour</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>theta_current <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(theta)), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>et d’un optimiseur:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>theta_optimizer <span class="ot">&lt;-</span> <span class="fu">optim_rprop</span>(theta_current)</span></code></pre></div>
<p>On considère ici l’optimiseur Rprop (resilient backpropagation) qui
ne prend pas en compte l’amplitude du gradient mais uniquement le signe
de ses coordonnées (voir <a href="https://florian.github.io/rprop/">ici
pour une introduction pédagogique à Rprop</a>).</p>
<p>Intuitivement, l’optimiseur a juste besoin de la valeur de <span
class="math inline">\(\theta\)</span> et de son gradient pour le mettre
à jour. Mais à ce stade on ne connaît pas encore le gradient <span
class="math inline">\(\nabla_\theta \mathcal{L}(\mathbf{X}, \mathbf{y};
\theta)\)</span></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>theta_current<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<p>et il faut donc le calculer:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">&lt;-</span> <span class="fu">eval_loss</span>(theta_current, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>loss<span class="sc">$</span><span class="fu">backward</span>()</span></code></pre></div>
<p>On peut vérifier que le gradient est stocké dans
<code>theta</code></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>theta_current<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## -28.5439
##  14.4407
##   9.6938
## [ CPUFloatType{3} ]</code></pre>
<p>et effectuer la mise à jour avec une étape d’optimisation</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>theta_optimizer<span class="sc">$</span><span class="fu">step</span>()</span></code></pre></div>
<pre><code>## NULL</code></pre>
<p>On peut vérifier que le paramètre courant a été mis à jour.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>theta_current</span></code></pre></div>
<pre><code>## torch_tensor
## 0.001 *
## 10.0000
## -10.0000
## -10.0000
## [ CPUFloatType{3} ][ requires_grad = TRUE ]</code></pre>
<p>Il ne reste plus qu’à recommencer pour un nombre d’itérations donné.
<strong>Attention,</strong> il faut réinitialiser le gradient avant de
le mettre à jour, le comportement par défaut de mise à jour étant
l’<em>accumulation</em> plutôt que le <em>remplacement</em>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>loss_vector <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="at">length =</span> num_iterations)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  theta_optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> <span class="fu">eval_loss</span>(theta_current, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>  theta_optimizer<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>  loss_vector[i] <span class="ot">&lt;-</span> loss <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>On vérifie que la perte diminue au cours du temps.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>num_iterations, loss_vector)</span></code></pre></div>
<p><img src="autodiff_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>On constate que notre optimiseur aboutit au même résultat que
<code>glm()</code></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">torch =</span> theta_current <span class="sc">|&gt;</span> <span class="fu">as.numeric</span>(),</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">glm   =</span> <span class="fu">glm</span>(Y <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> X, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>) <span class="sc">|&gt;</span> <span class="fu">coefficients</span>()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##    torch    glm
##    &lt;dbl&gt;  &lt;dbl&gt;
## 1  1.79   1.79 
## 2 -1.04  -1.04 
## 3 -0.973 -0.973</code></pre>
<p><strong>Attention</strong> la mécanique présentée ci-dessus avec
<code>$step()</code> ne fonctionne pas pour certaines routines
d’optimisation (BFGS, gradient conjugué) qui nécessite de calculer
plusieurs fois la fonction objective. Dans ce cas, il faut définir une
<em>closure</em>, qui renvoie la fonction objective, et la passer en
argument à <code>$step()</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">## remise à 0 du paramètre courant</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>theta_current <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(theta)), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>theta_optimizer <span class="ot">&lt;-</span> <span class="fu">optim_rprop</span>(theta_current)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="do">## définition de la closure</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>calc_loss <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>  theta_optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> <span class="fu">eval_loss</span>(theta_current, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>  loss</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Optimisation avec la closure</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>loss_vector <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="at">length =</span> num_iterations)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>  loss_vector[i] <span class="ot">&lt;-</span> theta_optimizer<span class="sc">$</span><span class="fu">step</span>(calc_loss) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>On peut vérifier qu’on obtient des résultats identiques dans les deux
cas d’utilisation:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>theta_current</span></code></pre></div>
<pre><code>## torch_tensor
##  1.7915
## -1.0380
## -0.9729
## [ CPUFloatType{3} ][ requires_grad = TRUE ]</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>num_iterations, loss_vector)</span></code></pre></div>
<p><img src="autodiff_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="exemple-de-régression-multivariée" class="section level2">
<h2>Exemple de régression multivariée</h2>
<p>On considère un exemple de régression multiple, réalisé à partir du
blog <a
href="https://blogs.rstudio.com/ai/posts/2021-04-22-torch-for-optimization/">torch
for optimization</a>, où l’on cherche à estimer les paramètres de
moyenne ainsi que la variance par maximisation de la vraisemblance.</p>
<p>On génère les données</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate the data</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">100</span>),<span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">1000</span>),<span class="dv">100</span>,<span class="dv">10</span>))</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>Beta.true <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">11</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>Beta.true <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span></code></pre></div>
<p>La fonction de perte à optimiser (ici la log-vraisemblance) va
dépendre d’inputs définis comme des “tenseurs torch”:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Declare the loss function</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>Theta <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">12</span>) <span class="sc">%&gt;%</span> as.matrix, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>X.tensor <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(X)</span></code></pre></div>
<p>Quelques remarques :</p>
<p>- le paramètre <span class="math inline">\(\theta\)</span> à
optimiser est ici défini comme un tenseur, i.e. un objet qui va
notamment stocker la valeur courante de <span
class="math inline">\(\theta\)</span>. Avec l’option
“requires_grad=TRUE” la valeur courante du gradient de la dernière
fonction appelée dépendant de <span
class="math inline">\(\theta\)</span> va aussi être stockée.</p>
<p>- la matrice <span class="math inline">\(X\)</span> est aussi définie
comme un tenseur, mais l’option “requires_grad=TRUE” n’a pas été
spécifiée, le gradient ne sera donc pas stocké pour cet objet. Cette
distinction est explicitée lorsque l’on affiche les deux objets:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>Theta[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>## torch_tensor
##  1
##  1
##  1
## [ CPUFloatType{3,1} ][ grad_fn = &lt;SliceBackward0&gt; ]</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>X.tensor[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>## torch_tensor
##  1.0000 -0.7146  0.4371
##  1.0000  0.1269  1.9857
##  1.0000 -0.5624  0.2708
## [ CPUFloatType{3,3} ]</code></pre>
<p>La fonction de perte est ici la log-vraisemblance, elle-même définie
à partir d’opérateurs torch élémentaires :</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>LogLik <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">torch_mul</span>(n,<span class="fu">torch_log</span>(Theta[<span class="dv">12</span>])) <span class="sc">+</span> <span class="fu">torch_square</span>(<span class="fu">torch_norm</span>(Y<span class="sc">-</span><span class="fu">torch_matmul</span>(X.tensor,Theta[<span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>])))<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span><span class="fu">torch_square</span>(Theta[<span class="dv">12</span>]))</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>La fonction LogLik peut être appliquée comme une fonction R qui
prendra directement en argument les valeurs courantes de X.tensor et
<span class="math inline">\(\theta\)</span>, et produira en sortie un
tenseur</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">LogLik</span>()</span></code></pre></div>
<pre><code>## torch_tensor
##  1969.8533
## [ CPUFloatType{1} ][ grad_fn = &lt;AddBackward0&gt; ]</code></pre>
<p>Outre la valeur courante de la fonction, ce tenseur contient la
“recette” du graphe computationnel utilisé dans calcul backward du
gradient de la fonction LogLik par rapport à <span
class="math inline">\(\theta\)</span>. On peut ainsi afficher la
dernière opération de ce graphe</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>toto <span class="ot">&lt;-</span> <span class="fu">LogLik</span>()</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>toto<span class="sc">$</span>grad_fn</span></code></pre></div>
<pre><code>## AddBackward0</code></pre>
<p>correspondant à l’addition (AddBackward) des deux termes <span
class="math display">\[  n\times \log(\theta[12]) \quad \text{et} \quad
||Y-X\theta[1:11]||^2/(2*\theta[12]^2)\]</span> dans le calcul de la
perte. On peut afficher les opérations suivantes dans le graphe comme
suit:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>toto<span class="sc">$</span>grad_fn<span class="sc">$</span>next_functions</span></code></pre></div>
<pre><code>## [[1]]
## MulBackward0
## [[2]]
## DivBackward0</code></pre>
<p>L’étape suivante consiste à choisir la méthode d’optimisation à
appliquer. L’intérêt d’utiliser le package `{{torch}}` est d’avoir accès
à une large gamme de méthodes d’optimisation, on considère ici la
méthode rprop qui réalise une descente de gradient à pas adaptatif et
spécifique à chaque coordonnée:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Specify the optimization parameters</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_rprop</span>(Theta,lr)</span></code></pre></div>
<p>On décrit maintenant un pas de calcul du gradient, contenant les
étapes suivantes : - réinitialisation du gradient de <span
class="math inline">\(\theta\)</span>,<br />
- évaluation de la fonction de perte (avec la valeur courante de <span
class="math inline">\(\theta\)</span>),<br />
- calcul backward du gradient. On inclut tout cela dans une
fonction:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Optimization step description</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>calc_loss <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>  optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>  value <span class="ot">&lt;-</span> <span class="fu">LogLik</span>()</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>  value</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Commençons par regarder ce que fait concrètement cette fonction.
L’état courant du paramètre est le suivant:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>Theta</span></code></pre></div>
<pre><code>## torch_tensor
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
## [ CPUFloatType{12,1} ][ requires_grad = TRUE ]</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>Theta<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<p>On applique une première fois la fonction, et on obtient la mise à
jour suivante :</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">calc_loss</span>()</span></code></pre></div>
<pre><code>## torch_tensor
##  1969.8533
## [ CPUFloatType{1} ][ grad_fn = &lt;AddBackward0&gt; ]</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>Theta</span></code></pre></div>
<pre><code>## torch_tensor
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
##  1
## [ CPUFloatType{12,1} ][ requires_grad = TRUE ]</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>Theta<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
##  322.0468
##  167.9877
##  279.3360
##  116.2697
##    8.5126
##  241.4523
##  102.8678
##  159.1056
## -123.2218
##  323.6690
##  238.3180
## -3839.7065
## [ CPUFloatType{12,1} ]</code></pre>
<p>Comme on le voit la valeur courante du paramètre n’a pas changée, en
revanche Theta$grad contient maintenant le gradient de la fonction de
perte calculé en <span class="math inline">\(\theta\)</span>. Dans le
cas où la méthode d’optimisation considérée n’a besoin que de la valeur
courante du gradient et du paramètre, on peut directement faire la mise
à jour de <span class="math inline">\(\theta\)</span> :</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">step</span>()</span></code></pre></div>
<pre><code>## NULL</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>Theta</span></code></pre></div>
<pre><code>## torch_tensor
##  0.9900
##  0.9900
##  0.9900
##  0.9900
##  0.9900
##  0.9900
##  0.9900
##  0.9900
##  1.0100
##  0.9900
##  0.9900
##  1.0100
## [ CPUFloatType{12,1} ][ requires_grad = TRUE ]</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>Theta<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
##  322.0468
##  167.9877
##  279.3360
##  116.2697
##    8.5126
##  241.4523
##  102.8678
##  159.1056
## -123.2218
##  323.6690
##  238.3180
## -3839.7065
## [ CPUFloatType{12,1} ]</code></pre>
<p>Il n’y a plus qu’à itérer !</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Run the optimization</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>LogLik.hist <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num_iterations)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>  LogLik.hist[i] <span class="ot">&lt;-</span> <span class="fu">calc_loss</span>() <span class="sc">%&gt;%</span> as.numeric</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>  optimizer<span class="sc">$</span><span class="fu">step</span>() </span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>On vérifie que l’optimisation s’est bien passée (ie que l’on a
minimisé la fonction de perte)</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="do">## How does the loss function behave ?</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LogLik.hist,<span class="at">cex=</span><span class="fl">0.4</span>)</span></code></pre></div>
<p><img src="autodiff_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Are the gradients at 0 ?</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>Theta<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## 0.0001 *
## -0.5746
##  -0.0858
##   1.5259
##  -1.2350
##   0.7248
##   1.3638
##   1.0169
##   0.9131
##   0.2003
##   1.0300
##   1.2445
##  -0.9155
## [ CPUFloatType{12,1} ]</code></pre>
<p>et que le résultat est comparable à la solution classique obtenue par
OLS :</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare the coef estimates with the ones of lm </span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>Res <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X[,<span class="sc">-</span><span class="dv">1</span>] )</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Res<span class="sc">$</span>coefficients,<span class="fu">as.numeric</span>(Theta)[<span class="sc">-</span><span class="dv">12</span>])</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="dv">0</span>,<span class="at">b=</span><span class="dv">1</span>,<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="autodiff_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare the variances</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Res)<span class="sc">$</span>sigma<span class="sc">**</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.8109918</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>Theta[<span class="dv">12</span>]</span></code></pre></div>
<pre><code>## torch_tensor
##  0.8496
## [ CPUFloatType{1} ][ grad_fn = &lt;SelectBackward0&gt; ]</code></pre>
</div>
</div>
<div id="fonctions-de-torch-compatibles-avec-autograd"
class="section level1">
<h1>Fonctions de torch compatibles avec autograd</h1>
<div id="fonctions-usuelles-définies-dans-torch" class="section level2">
<h2>Fonctions usuelles définies dans torch</h2>
<ul>
<li><p><code>torch_ones(NB_OF_ROWS, NB_OF_COLS, requires_grad = TRUE)</code>
ou <code>torch_ones(VECTOR_OF_DIMS, requires_grad = TRUE)</code> : crée
un tenseur rempli de 1.</p></li>
<li><p><code>torch_tensor(OBJECT, requires_grad = TRUE)</code> :
convertit un objet R (vecteur, matrice ou array) en un tenseur
torch.</p></li>
<li><p><code>$mean()</code>, <code>$sum()</code>,
<code>$pow(ORDER)</code>, <code>$mm(MATRIX)</code> (matrix
multiplication), cf les fonctions torch_* de la documentation R de
torch.</p></li>
<li><p>Les fonctions de torch peuvent s’utiliser de manière analogue aux
pipes. Par exemple,
<code>x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)</code>.</p></li>
<li><p><code>output$backward()</code> : effectue la propagation backward
pour calculer les gradients successifs.</p></li>
<li><p><code>input$grad</code> : récupère le gradient de
<code>output</code> par rapport à <code>input</code>.</p></li>
<li><p><code>midput$retain_grad()</code> : à lancer avant `<span
class="math inline">\(backward()\)</span> et permet de stocker les
gradients intermédiaires.</p></li>
</ul>
</div>
<div id="comment-implémenter-une-fonction-compatible-avec-autograd"
class="section level2">
<h2>Comment implémenter une fonction compatible avec autograd ?</h2>
<p>Une fonction compatible avec autograd doit être définie comme une
classe via l’opérateur <code>autograd_function</code>, et posséder deux
méthodes, <code>forward</code> et <code>backward</code>, qui déterminent
respectivement quelle opération est exécutée par le code et comment
calculer le gradient.</p>
<ul>
<li><code>ctx</code> correspond aux objets partagés entre la méthode
<code>forward</code> et la méthode <code>backward</code>, et est
toujours le premier argument de ces deux méthodes.</li>
<li><code>$save_for_backward()</code> permet de sauvegarder des valeurs
des inputs et/ou outputs à utiliser lors du calcul du gradient dans la
méthode <code>backward</code>.</li>
</ul>
<p>Exemple de code :</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>log_base <span class="ot">=</span> <span class="fu">autograd_function</span>(</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">forward =</span> <span class="cf">function</span>(ctx, input, base) {</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>        ctx<span class="sc">$</span><span class="fu">save_for_backward</span>(<span class="at">input =</span> input, <span class="at">base =</span> base)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>        input<span class="sc">$</span><span class="fu">log</span>() <span class="sc">/</span> <span class="fu">log</span>(base)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">backward =</span> <span class="cf">function</span>(ctx, grad_output) {</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>        vars <span class="ot">=</span> ctx<span class="sc">$</span>saved_variables</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">list</span>(<span class="at">input =</span> grad_output <span class="sc">/</span> (vars<span class="sc">$</span>input <span class="sc">*</span> <span class="fu">log</span>(vars<span class="sc">$</span>base)))</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">log_base</span>(x, <span class="dv">10</span>)</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>y<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>x<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
##  0.2171
## [ CPUFloatType{1} ]</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">10</span>))</span></code></pre></div>
<pre><code>## [1] 0.2171472</code></pre>
<p><code>backward</code> doit être capable de propager le calcul du
gradient aux inputs de la fonction implémentée. Par exemple, en
considérant que la fonction implémentée correspond à la fonction <span
class="math inline">\(f\)</span> dans le chaînage <span
class="math display">\[x \xrightarrow[]{f} y \rightarrow z,\]</span> La
méthode <code>backward</code> de la fonction <span
class="math inline">\(f\)</span> doit être capable de sortir <span
class="math inline">\(\partial z / \partial x = (\partial y / \partial
x)(\partial z / \partial y)\)</span>, où dans l’exemple précédent <span
class="math inline">\(\partial z / \partial y\)</span> est représenté
par l’argument <code>grad_output</code>.</p>
</div>
</div>
<div id="un-exemple-pratique-le-modèle-poisson-lognormal"
class="section level1">
<h1>Un exemple pratique: le modèle Poisson Lognormal</h1>
<div id="le-modèle" class="section level3">
<h3>Le modèle</h3>
<p>Le modèle Poisson lognormal multivarié lie des vecteurs de comptages
<span class="math inline">\(p\)</span>-dimensionnel <span
class="math inline">\(\mathbf{Y}_i\)</span> observés à des vecteurs
gaussiens <span class="math inline">\(p\)</span>-dimensionnel latents
<span class="math inline">\(\mathbf{Z}_i\)</span> comme suit</p>
<span class="math display">\[\begin{equation}
  \begin{array}{rcl}
  \text{espace latent} &amp;   \mathbf{Z}_i \sim
\mathcal{N}({\boldsymbol 0},\boldsymbol\Sigma) , \\
  \text{espace des observations} &amp;  Y_{ij} | Z_{ij} \quad
\text{indep.} &amp;   \mathbf{Y}_i |
\mathbf{Z}_i\sim\mathcal{P}\left(\exp\{{\mathbf{o}_i +
\mathbf{x}_i^\top\boldsymbol B} + \mathbf{Z}_i\}\right).
  \end{array}
\end{equation}\]</span>
<p>L’effet principal est dû à une combinaison linéaire de <span
class="math inline">\(d\)</span> covariables <span
class="math inline">\(\mathbf{x}_i\)</span> (includant un vecteur de
constantes). Le vecteur fixé <span
class="math inline">\(\mathbf{o}_i\)</span> correspond à un vecteur
d’offsets, c’est-à-dire un effet connu est fixé sur les <span
class="math inline">\(p\)</span> variables dans chaque échantillon. Les
paramètres à estimer sont la matrice des coefficients de régression
<span class="math inline">\(\boldsymbol B\)</span> et la matrice de
covariance <span class="math inline">\(\boldsymbol\Sigma =
\mathbf{\Omega}^{-1}\)</span> décrivant la structure de dépendance
résiduelle entre les <span class="math inline">\(p\)</span> variables
dans l’espace latent. On note <span class="math inline">\(\theta =
({\boldsymbol B}, \mathbf{\Omega})\)</span> le vecteur des paramètres du
modèles.</p>
</div>
<div id="approximation-variationnelle" class="section level3">
<h3>Approximation variationnelle</h3>
<p>Une manière classique d’estimer l’ajustement de ce modèle consiste en
l’utilisation d’une approximation variationnelle de la vraisemblance,
appelée vraisemblance variationnelle ou ELBO (Evidence Lower Bound) qui
prend la forme suivante:</p>
<p><span class="math display">\[\begin{multline}
  \label{eq:elbo}
\mathcal{J}_n(\theta, \mathbf{\psi})
  = \mathrm{trace} ( \mathbf{Y}^\top [\mathbf{O} + \mathbf{M} +
\mathbf{X}\boldsymbol{B}]) -
\mathrm{trace}(\tilde{\mathbf{A}}^\top \mathbf{1}_{n,p}) + K(\mathbf{Y})
+ \frac{n}2\log|\mathbf{\Omega}|\\
- \frac12 \mathrm{trace}(\mathbf{M} \mathbf{\Omega} \mathbf{M}^\top)   -
\frac12 \mathrm{trace}(\bar{\mathbf{S}}^2 \mathbf{\Omega}) +
\mathrm{trace}(\log(\mathbf{S})^\top \mathbf{1}_{n,p}) + \frac12 np,
\end{multline}\]</span> où <span class="math inline">\(\mathbf{M} =
[\mathbf{m}_1, \dots, \mathbf{m}_n]^\top\)</span>, <span
class="math inline">\(\mathbf{S} = [\mathbf{s}_1, \dots,
\mathbf{s}_n]^\top\)</span>, <span
class="math inline">\(\bar{\mathbf{S}}^2 = \sum_{i=1}^n
\mathrm{diag}(\mathbf{s}_i\circ \mathbf{s}_{i})\)</span> sont des
paramètres additionnels dits “variationnels” gérant l’approximation de
la vraie loi conditionnelle de <span class="math inline">\(\mathbf{Z} |
\mathbf{Y}\)</span> et consécutivement de la log-vraisemblance. On note
<span class="math inline">\(\psi = (\mathbf{M}, \mathbf{S})\)</span>
l’ensemble de ces paramètres.</p>
<p>Notre approche consiste à utiliser une forme dite variationnelle de
l’algorithme EM alternant l’estimation des paramètres <span
class="math inline">\(\theta\)</span> et <span
class="math inline">\(\psi\)</span>:</p>
<span class="math display">\[\begin{align}
\label{eq:vem}
\mathrm{VE-step} &amp; \left( \hat{\psi}^{\text{ve}} \right) =
\arg\max_{\psi} \mathcal{J}_n (\psi, \theta)\\
\mathrm{M-step} &amp; \left(\hat{\theta} \right) = \arg\max_{\theta}
\mathcal{J}_n(\psi, \theta)\\
\end{align}\]</span>
</div>
<div id="optimiseur-classique" class="section level3">
<h3>Optimiseur classique</h3>
<p>L’optimiseur implémenté dans <strong>PLNmodels</strong> s’appuie sur
les gradients des paramètres <span class="math inline">\(\theta\)</span>
et <span class="math inline">\(\psi\)</span></p>
<span class="math display">\[\begin{align}
  \label{eq:derivatives-elbo-model}
  \begin{split}
    \nabla_{\mathbf{B}} J_n(\theta)  &amp; = \mathbf{X}^\top
(\mathbf{Y}  - \mathbf{A}), \\
    %
    \nabla_{\mathbf{\Omega}} J_n(\theta) &amp; = \frac{n}{2}
\left[  \mathbf{\Omega}^{-1} - \frac{\mathbf{M}^\top\mathbf{M} +
\bar{\mathbf{S}}^2}{n} \right] \Leftrightarrow \mathbf{\Omega}^{-1} =
\frac{1}{n}\left(\mathbf{M}^\top\mathbf{M} +
\bar{\mathbf{S}}^2\right)  \\
\nabla_{\mathbf{M}} J_n(\mathbf{\psi}) &amp; = \mathbf{Y} -
\tilde{\!\mathbf{A}} - \mathbf{M}\mathbf{\Omega} \\[1.5ex]
    %
\nabla_{\mathbf{S}} J_n(\mathbf{\psi}) &amp; = -\mathbf{S} \circ
\tilde{\!\mathbf{A}} + 1/\mathbf{S} -
\mathbf{S}\mathbf{D}_{\mathbf{\Omega}}\\[1.5ex]
    %
  \end{split}
\end{align}\]</span>
<p>où</p>
<span class="math display">\[\begin{equation*}
  \mathbf{A} = \exp\{\mathbf{O} + \mathbf{X}\mathbf{B} + \mathbf{M} +
\mathbf{S}^2/2\}.
\end{equation*}\]</span>
<p>On utilise le fait qu’il existe une forme explicite pour <span
class="math inline">\(\Sigma\)</span> dans l’étape M et on fait une
descente de gradient sur les autres paramtères <span
class="math inline">\((\mathbf{M}, \mathbf{S}, \mathbf{B})\)</span>.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(PLNmodels)</span></code></pre></div>
<pre><code>## This is packages &#39;PLNmodels&#39; version 0.11.7</code></pre>
<pre><code>## Use future::plan(multicore/multisession) to speed up PLNPCA/PLNmixture/stability_selection.</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(oaks)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>myPLN_classical <span class="ot">&lt;-</span> <span class="fu">PLN</span>(Abundance <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(Offset)), <span class="at">data =</span> oaks)</span></code></pre></div>
<pre><code>## 
##  Initialization...
##  Adjusting a PLN model with full covariance model
##  Post-treatments...
##  DONE!</code></pre>
</div>
<div id="utilisation-de-torch-et-lautodifférentiation"
class="section level3">
<h3>Utilisation de torch et l’autodifférentiation</h3>
<p>Avec torch, et pourvu qu’on détermine un bon optimiseur, il suffit de
spécifier l’ELBO avec les objets adéquats et faire le calcul de <span
class="math inline">\(\Sigma\)</span> de manière explicite.</p>
<p>On propose ci-dessous une implémentation dans une classe R6 (faite
avec Bastien et Mahendra).</p>
<div class="sourceCode" id="cb101"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(R6)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>log_stirling <span class="ot">&lt;-</span> <span class="cf">function</span>(n_){</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>  n_ <span class="ot">&lt;-</span> n_<span class="sc">+</span> (n_<span class="sc">==</span><span class="dv">0</span>)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">torch_log</span>(<span class="fu">torch_sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>n_)) <span class="sc">+</span> n_<span class="sc">*</span><span class="fu">log</span>(n_<span class="sc">/</span><span class="fu">exp</span>(<span class="dv">1</span>))</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>PLN <span class="ot">&lt;-</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">R6Class</span>(<span class="st">&quot;PLN&quot;</span>,</span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">public =</span> <span class="fu">list</span>(</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">Y =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">O =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">X =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">n =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">p =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a>      <span class="at">d =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a>      <span class="at">M =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a>      <span class="at">S =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">A =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-21"><a href="#cb101-21" aria-hidden="true" tabindex="-1"></a>      <span class="at">B =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-22"><a href="#cb101-22" aria-hidden="true" tabindex="-1"></a>      <span class="at">Sigma =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-23"><a href="#cb101-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">Omega =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-24"><a href="#cb101-24" aria-hidden="true" tabindex="-1"></a>      <span class="at">ELBO_list =</span> <span class="cn">NULL</span>,</span>
<span id="cb101-25"><a href="#cb101-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-26"><a href="#cb101-26" aria-hidden="true" tabindex="-1"></a>      <span class="do">## Constructor</span></span>
<span id="cb101-27"><a href="#cb101-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">initialize =</span> <span class="cf">function</span>(Y, O, X){</span>
<span id="cb101-28"><a href="#cb101-28" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(Y)</span>
<span id="cb101-29"><a href="#cb101-29" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>O <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(O)</span>
<span id="cb101-30"><a href="#cb101-30" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>X <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb101-31"><a href="#cb101-31" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb101-32"><a href="#cb101-32" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(Y)</span>
<span id="cb101-33"><a href="#cb101-33" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>d <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb101-34"><a href="#cb101-34" aria-hidden="true" tabindex="-1"></a>        <span class="do">## Variational parameters</span></span>
<span id="cb101-35"><a href="#cb101-35" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>M <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(self<span class="sc">$</span>n, self<span class="sc">$</span>p, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb101-36"><a href="#cb101-36" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>S <span class="ot">&lt;-</span> <span class="fu">torch_ones</span>(self<span class="sc">$</span>n , self<span class="sc">$</span>p, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb101-37"><a href="#cb101-37" aria-hidden="true" tabindex="-1"></a>        <span class="do">## Model parameters</span></span>
<span id="cb101-38"><a href="#cb101-38" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>B <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(self<span class="sc">$</span>d, self<span class="sc">$</span>p, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb101-39"><a href="#cb101-39" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>Sigma <span class="ot">&lt;-</span> <span class="fu">torch_eye</span>(self<span class="sc">$</span>p)</span>
<span id="cb101-40"><a href="#cb101-40" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>Omega <span class="ot">&lt;-</span> <span class="fu">torch_eye</span>(self<span class="sc">$</span>p)</span>
<span id="cb101-41"><a href="#cb101-41" aria-hidden="true" tabindex="-1"></a>        <span class="do">## Monitoring</span></span>
<span id="cb101-42"><a href="#cb101-42" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>ELBO_list <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb101-43"><a href="#cb101-43" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb101-44"><a href="#cb101-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-45"><a href="#cb101-45" aria-hidden="true" tabindex="-1"></a>      <span class="at">get_Sigma =</span> <span class="cf">function</span>(M, S){</span>
<span id="cb101-46"><a href="#cb101-46" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span><span class="sc">/</span>self<span class="sc">$</span>n <span class="sc">*</span> (<span class="fu">torch_matmul</span>(<span class="fu">torch_transpose</span>(M,<span class="dv">2</span>,<span class="dv">1</span>),M) <span class="sc">+</span> <span class="fu">torch_diag</span>(<span class="fu">torch_sum</span>(<span class="fu">torch_multiply</span>(S,S), <span class="at">dim =</span> <span class="dv">1</span>)))</span>
<span id="cb101-47"><a href="#cb101-47" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb101-48"><a href="#cb101-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-49"><a href="#cb101-49" aria-hidden="true" tabindex="-1"></a>      <span class="at">get_ELBO =</span> <span class="cf">function</span>(B, M, S, Omega){</span>
<span id="cb101-50"><a href="#cb101-50" aria-hidden="true" tabindex="-1"></a>        S2 <span class="ot">&lt;-</span> <span class="fu">torch_multiply</span>(S, S)</span>
<span id="cb101-51"><a href="#cb101-51" aria-hidden="true" tabindex="-1"></a>        XB <span class="ot">&lt;-</span> <span class="fu">torch_matmul</span>(self<span class="sc">$</span>X, B)</span>
<span id="cb101-52"><a href="#cb101-52" aria-hidden="true" tabindex="-1"></a>        A  <span class="ot">&lt;-</span> <span class="fu">torch_exp</span>(self<span class="sc">$</span>O <span class="sc">+</span> M <span class="sc">+</span> XB <span class="sc">+</span> S2<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb101-53"><a href="#cb101-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-54"><a href="#cb101-54" aria-hidden="true" tabindex="-1"></a>        elbo <span class="ot">&lt;-</span> n<span class="sc">/</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">torch_logdet</span>(Omega)</span>
<span id="cb101-55"><a href="#cb101-55" aria-hidden="true" tabindex="-1"></a>        elbo <span class="ot">&lt;-</span> <span class="fu">torch_add</span>(elbo, <span class="fu">torch_sum</span>(<span class="sc">-</span> A <span class="sc">+</span> <span class="fu">torch_multiply</span>(self<span class="sc">$</span>Y, self<span class="sc">$</span>O <span class="sc">+</span> M <span class="sc">+</span> XB) <span class="sc">+</span> .<span class="dv">5</span> <span class="sc">*</span> <span class="fu">torch_log</span>(S2)))</span>
<span id="cb101-56"><a href="#cb101-56" aria-hidden="true" tabindex="-1"></a>        elbo <span class="ot">&lt;-</span> <span class="fu">torch_sub</span>(elbo, .<span class="dv">5</span> <span class="sc">*</span> <span class="fu">torch_trace</span>(<span class="fu">torch_matmul</span>(<span class="fu">torch_matmul</span>(<span class="fu">torch_transpose</span>(M, <span class="dv">2</span>, <span class="dv">1</span>), M) <span class="sc">+</span> <span class="fu">torch_diag</span>(<span class="fu">torch_sum</span>(S2, <span class="at">dim =</span> <span class="dv">1</span>)), Omega)))</span>
<span id="cb101-57"><a href="#cb101-57" aria-hidden="true" tabindex="-1"></a>        elbo <span class="ot">&lt;-</span> <span class="fu">torch_add</span>(elbo, .<span class="dv">5</span> <span class="sc">*</span> self<span class="sc">$</span>n <span class="sc">*</span> self<span class="sc">$</span>p <span class="sc">-</span> <span class="fu">torch_sum</span>(<span class="fu">log_stirling</span>(self<span class="sc">$</span>Y)))</span>
<span id="cb101-58"><a href="#cb101-58" aria-hidden="true" tabindex="-1"></a>        elbo</span>
<span id="cb101-59"><a href="#cb101-59" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb101-60"><a href="#cb101-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-61"><a href="#cb101-61" aria-hidden="true" tabindex="-1"></a>      <span class="at">fit =</span> <span class="cf">function</span>(N_iter, lr, <span class="at">tol =</span> <span class="fl">1e-8</span>, <span class="at">verbose =</span> <span class="cn">FALSE</span>){</span>
<span id="cb101-62"><a href="#cb101-62" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>ELBO_list <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="at">length =</span> N_iter)</span>
<span id="cb101-63"><a href="#cb101-63" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="ot">&lt;-</span> <span class="fu">optim_rprop</span>(<span class="fu">c</span>(self<span class="sc">$</span>B, self<span class="sc">$</span>M, self<span class="sc">$</span>S), <span class="at">lr =</span> lr)</span>
<span id="cb101-64"><a href="#cb101-64" aria-hidden="true" tabindex="-1"></a>        objective0 <span class="ot">&lt;-</span> <span class="cn">Inf</span></span>
<span id="cb101-65"><a href="#cb101-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_iter){</span>
<span id="cb101-66"><a href="#cb101-66" aria-hidden="true" tabindex="-1"></a>          <span class="do">## reinitialize gradients</span></span>
<span id="cb101-67"><a href="#cb101-67" aria-hidden="true" tabindex="-1"></a>          optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb101-68"><a href="#cb101-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-69"><a href="#cb101-69" aria-hidden="true" tabindex="-1"></a>          <span class="do">## compute current ELBO</span></span>
<span id="cb101-70"><a href="#cb101-70" aria-hidden="true" tabindex="-1"></a>          loss <span class="ot">&lt;-</span> <span class="sc">-</span> self<span class="sc">$</span><span class="fu">get_ELBO</span>(self<span class="sc">$</span>B, self<span class="sc">$</span>M, self<span class="sc">$</span>S, self<span class="sc">$</span>Omega)</span>
<span id="cb101-71"><a href="#cb101-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-72"><a href="#cb101-72" aria-hidden="true" tabindex="-1"></a>          <span class="do">## backward propagation and optimization</span></span>
<span id="cb101-73"><a href="#cb101-73" aria-hidden="true" tabindex="-1"></a>          loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb101-74"><a href="#cb101-74" aria-hidden="true" tabindex="-1"></a>          optimizer<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb101-75"><a href="#cb101-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-76"><a href="#cb101-76" aria-hidden="true" tabindex="-1"></a>          <span class="do">## update parameters with close form</span></span>
<span id="cb101-77"><a href="#cb101-77" aria-hidden="true" tabindex="-1"></a>          self<span class="sc">$</span>Sigma <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">get_Sigma</span>(self<span class="sc">$</span>M, self<span class="sc">$</span>S)</span>
<span id="cb101-78"><a href="#cb101-78" aria-hidden="true" tabindex="-1"></a>          self<span class="sc">$</span>Omega <span class="ot">&lt;-</span> <span class="fu">torch_inverse</span>(self<span class="sc">$</span>Sigma)</span>
<span id="cb101-79"><a href="#cb101-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-80"><a href="#cb101-80" aria-hidden="true" tabindex="-1"></a>          objective <span class="ot">&lt;-</span> <span class="sc">-</span>loss<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb101-81"><a href="#cb101-81" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span>(verbose <span class="sc">&amp;&amp;</span> (i <span class="sc">%%</span> <span class="dv">50</span> <span class="sc">==</span> <span class="dv">0</span>)){</span>
<span id="cb101-82"><a href="#cb101-82" aria-hidden="true" tabindex="-1"></a>            <span class="fu">pr</span>(<span class="st">&#39;i : &#39;</span>, i )</span>
<span id="cb101-83"><a href="#cb101-83" aria-hidden="true" tabindex="-1"></a>            <span class="fu">pr</span>(<span class="st">&#39;ELBO&#39;</span>, objective)</span>
<span id="cb101-84"><a href="#cb101-84" aria-hidden="true" tabindex="-1"></a>          }</span>
<span id="cb101-85"><a href="#cb101-85" aria-hidden="true" tabindex="-1"></a>          self<span class="sc">$</span>ELBO_list[i] <span class="ot">&lt;-</span> objective</span>
<span id="cb101-86"><a href="#cb101-86" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> (<span class="fu">abs</span>(objective0 <span class="sc">-</span> objective)<span class="sc">/</span><span class="fu">abs</span>(objective) <span class="sc">&lt;</span> tol) {</span>
<span id="cb101-87"><a href="#cb101-87" aria-hidden="true" tabindex="-1"></a>            self<span class="sc">$</span>ELBO_list <span class="ot">&lt;-</span> self<span class="sc">$</span>ELBO_list[<span class="dv">1</span><span class="sc">:</span>i]</span>
<span id="cb101-88"><a href="#cb101-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb101-89"><a href="#cb101-89" aria-hidden="true" tabindex="-1"></a>          } <span class="cf">else</span> {</span>
<span id="cb101-90"><a href="#cb101-90" aria-hidden="true" tabindex="-1"></a>            objective0 <span class="ot">&lt;-</span> objective</span>
<span id="cb101-91"><a href="#cb101-91" aria-hidden="true" tabindex="-1"></a>          }</span>
<span id="cb101-92"><a href="#cb101-92" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb101-93"><a href="#cb101-93" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb101-94"><a href="#cb101-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-95"><a href="#cb101-95" aria-hidden="true" tabindex="-1"></a>      <span class="at">plotLogNegElbo =</span> <span class="cf">function</span>(<span class="at">from =</span> <span class="dv">10</span>){</span>
<span id="cb101-96"><a href="#cb101-96" aria-hidden="true" tabindex="-1"></a>        <span class="fu">plot</span>(<span class="fu">log</span>(<span class="sc">-</span>self<span class="sc">$</span>ELBO_list[from<span class="sc">:</span><span class="fu">length</span>(self<span class="sc">$</span>ELBO_list) ]), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span id="cb101-97"><a href="#cb101-97" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb101-98"><a href="#cb101-98" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb101-99"><a href="#cb101-99" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Ça torche….</p>
<div class="sourceCode" id="cb102"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> oaks<span class="sc">$</span>Abundance</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(Y)))</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">log</span>(oaks<span class="sc">$</span>Offset)</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>myPLN <span class="ot">&lt;-</span> PLN<span class="sc">$</span><span class="fu">new</span>(<span class="at">Y =</span> Y, <span class="at">O =</span> <span class="fu">log</span>(O), <span class="at">X =</span> X)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>myPLN<span class="sc">$</span><span class="fu">fit</span>(<span class="dv">30</span>, <span class="at">lr =</span> <span class="fl">0.1</span>, <span class="at">tol =</span> <span class="fl">1e-5</span>)</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>myPLN<span class="sc">$</span><span class="fu">plotLogNegElbo</span>()</span></code></pre></div>
<p><img src="autodiff_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>Des temps de calcul similaires sur ce jeu de données (<span
class="math inline">\(n=116, p=114\)</span>)</p>
<div class="sourceCode" id="cb103"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>(</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>  myPLN_classical <span class="ot">&lt;-</span> <span class="fu">PLN</span>(Abundance <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(Offset)), <span class="at">data =</span> oaks,</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>                         <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="dv">0</span>)))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   3.269   0.763   2.034</code></pre>
<div class="sourceCode" id="cb105"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>  myPLN <span class="ot">&lt;-</span> PLN<span class="sc">$</span><span class="fu">new</span>(<span class="at">Y =</span> Y, <span class="at">O =</span> <span class="fu">log</span>(O), <span class="at">X =</span> X)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>  myPLN<span class="sc">$</span><span class="fu">fit</span>(<span class="dv">30</span>, <span class="fl">0.1</span>, <span class="at">tol =</span> <span class="fl">1e-5</span>)}</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   1.143   0.280   0.719</code></pre>
<p>On est proche en terme d’objectif</p>
<div class="sourceCode" id="cb107"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>myPLN_classical<span class="sc">$</span>loglik</span></code></pre></div>
<pre><code>## [1] -32248.93</code></pre>
<div class="sourceCode" id="cb109"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">last</span>(myPLN<span class="sc">$</span>ELBO_list)</span></code></pre></div>
<pre><code>## [1] -32278.62</code></pre>
</div>
</div>
<div id="les-modules-torch" class="section level1">
<h1>Les modules <code>torch</code></h1>
<p>En <code>keras</code> on différencie entre modèle et couche, en
<code>torch</code> tout est dans <code>nn_Module()</code>, classe de
base pour tous les modules de réseaux de neurones. Les modules peuvent
aussi contenir des sous-modules. Il existe un certain nombre de modules
pré-définis.</p>
<div id="un-module-de-base-layer" class="section level2">
<h2>Un module de base (‘layer’)</h2>
<p>Exemple d’un module linéaire :</p>
<div class="sourceCode" id="cb111"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>mylinearmod <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>mylinearmod</span></code></pre></div>
<pre><code>## An `nn_module` containing 4 parameters.
## 
## ── Parameters ──────────────────────────────────────────────────────────────────
## • weight: Float [1:1, 1:3]
## • bias: Float [1:1]</code></pre>
<p>Ce module a des paramètres de poids et biais. Le fait d’appeler un
module lance un <code>backward</code>. Plus besoin de mettre un
<code>requires_grad = TRUE</code>, c’est compris directement dans les
modules.</p>
<div class="sourceCode" id="cb113"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>data  <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">10</span>, <span class="dv">3</span>)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">mylinearmod</span>(data)</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>out</span></code></pre></div>
<pre><code>## torch_tensor
## -1.0566
## -0.1507
##  0.2706
## -0.9130
## -0.3506
## -2.1632
## -0.4442
##  0.1292
##  0.2301
## -0.5321
## [ CPUFloatType{10,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]</code></pre>
<div class="sourceCode" id="cb115"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span>grad_fn</span></code></pre></div>
<pre><code>## AddmmBackward0</code></pre>
<div class="sourceCode" id="cb117"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>mylinearmod<span class="sc">$</span>weight<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<div class="sourceCode" id="cb119"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>mylinearmod<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
## [ Tensor (undefined) ]</code></pre>
<p>On n’a pas encore fait de parcours backward, les valeurs sont donc
non définies. Si on lance <code>out$backward(),</code> on obtient une
erreur, due au fait qu’autograd attend en sortie un scalaire et qu’ici
nous avons un tenseur de (10, 1). Une façon de le corriger :</p>
<div class="sourceCode" id="cb121"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>d_avg_d_out <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="st">`</span><span class="at">repeat</span><span class="st">`</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">1</span>)<span class="sc">$</span><span class="fu">t</span>()</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span><span class="fu">backward</span>(<span class="at">gradient =</span> d_avg_d_out)</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>mylinearmod<span class="sc">$</span>weight<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
##  18.4731 -44.0767  -5.9814
## [ CPUFloatType{1,3} ]</code></pre>
<div class="sourceCode" id="cb123"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>mylinearmod<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>## torch_tensor
##  100
## [ CPUFloatType{1} ]</code></pre>
</div>
<div id="modèle" class="section level2">
<h2>Modèle</h2>
<p>C’est un module qui contient des modules. Par exemple</p>
<div class="sourceCode" id="cb125"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(<span class="dv">3</span>, <span class="dv">16</span>),</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_relu</span>(),</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(<span class="dv">16</span>, <span class="dv">1</span>)</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>model</span></code></pre></div>
<pre><code>## An `nn_module` containing 81 parameters.
## 
## ── Modules ─────────────────────────────────────────────────────────────────────
## • 0: &lt;nn_linear&gt; #64 parameters
## • 1: &lt;nn_relu&gt; #0 parameters
## • 2: &lt;nn_linear&gt; #17 parameters</code></pre>
<div class="sourceCode" id="cb127"><pre
class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># application</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">10</span>, <span class="dv">3</span>)</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">model</span>(data)</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>output</span></code></pre></div>
<pre><code>## torch_tensor
## -0.4635
## -0.1857
## -0.0988
## -0.2565
## -0.6305
## -0.6723
## -0.5323
## -0.3710
## -0.4241
## -0.4246
## [ CPUFloatType{10,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
