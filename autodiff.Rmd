---
title: "autodiff"
author: "finistere2022"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exploration de `{{torch}}` pour la différentiation automatique

## Installation 

Le package [`torch`]() permet de faire de la différentiation automatique à condition de réécrire son code avec les fonctions de `torch`. Il est basé sur la libraire C++ `libtorch` fourni dans PyTorch. Contrairement à d'anciennes versions, torch ne **fait pas** appel à python et ne nécessite pas reticulate. L'installation en est d'autant plus simple: 

```{r, eval = FALSE}
install.packages(torch)
```

Lors de la première utilisation, `torch` vous demandera de télécharger des fichiers supplémentaires via: 

```{r}
torch::install_torch() ## si vous avez un GPU compatible avec CUDA
## torch::install_torch(type = "cpu") ## sinon
```

`torch` est surtout utilisé pour des applications en ML/IA mais on peut aussi l'utiliser pour des calculs de gradients, hessiennes et de l'optimisation dans des modèles plus simples. On va l'illustrer ici pour de la régression logistique. 

## Régression logistique avec `torch`

On va adopter un simple modèle de régression logistique:

$$
Y_i \sim \mathcal{B}(\sigma(\theta^T x_i)) \quad \text{avec} \quad \sigma(x) = \frac{1}{1 + e^{-x}}
$$

Le but est d'estimer $\theta$ et éventuellement les erreurs associées. On commence par générer des données. 

```{r}
set.seed(42)
n <- 100
p <- 5
X <- matrix(rnorm(n = n*p), ncol = p, nrow = n)
beta <- seq(-2, 2, length.out = p)
probs <- (X %*% beta) %>% as.vector()
Y <- rbernoulli(n = n, p = probs) + 0L
```

