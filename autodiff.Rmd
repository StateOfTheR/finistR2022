---
title: "autodiff"
author: "finistere2022"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exploration de `{{torch}}` pour la différentiation automatique

## Installation 

Le package [`torch`]() permet de faire de la différentiation automatique à condition de réécrire son code avec les fonctions de `torch`. Il est basé sur la libraire C++ `libtorch` fourni dans PyTorch. Contrairement à d'anciennes versions, torch ne **fait pas** appel à python et ne nécessite pas reticulate. L'installation en est d'autant plus simple: 

```{r, eval = FALSE}
install.packages(torch)
```

Lors de la première utilisation, `torch` vous demandera de télécharger des fichiers supplémentaires via: 

```{r}
torch::install_torch() ## si vous avez un GPU compatible avec CUDA
## torch::install_torch(type = "cpu") ## sinon
```

`torch` est surtout utilisé pour des applications en ML/IA mais on peut aussi l'utiliser pour des calculs de gradients, hessiennes et de l'optimisation dans des modèles plus simples. On va l'illustrer ici pour de la régression logistique. 

```{r}
library(torch)
```


## Régression logistique avec `torch`

On va adopter un simple modèle de régression logistique:

$$
Y_i \sim \mathcal{B}(\sigma(\theta^T x_i)) \quad \text{avec} \quad \sigma(x) = \frac{1}{1 + e^{-x}}
$$

Le but est d'estimer $\theta$ et éventuellement les erreurs associées. On commence par générer des données. 

```{r}
set.seed(42)
n <- 100
p <- 5
X <- matrix(rnorm(n = n*p), ncol = p, nrow = n)
theta <- seq(-2, 2, length.out = p)
probs <- (X %*% theta) %>% as.vector()
Y <- rbernoulli(n = n, p = probs) + 0.
```

`torch` fonctionne avec ses propres types numériques, qu'il faut créer avec la fonction `torch_tensor()`.

```{r}
x <- torch_tensor(X)
y <- torch_tensor(Y)
```

On écrit ensuite la fonction de vraisemblance

```{r}
logistic_loss <- function(theta, x, y) {
  if (!is(theta, "torch_tensor")) {
    stop("theta must be a torch tensor")
  }
  odds <- torch_matmul(x, theta)
  log_lik <- torch_dot(y, odds) - torch_sum(torch_log(1 + torch_exp(odds)))
  return(log_lik)
}
```

avant de vérifier qu'elle fonctionne:

```{r}
logistic_loss(theta = torch_tensor(theta), x = x, y = y)
```

On veut ensuite définir une fonction objective à maximiser (qui ne dépend que de `theta`):

```{r}
eval_loss <- function(theta, verbose = TRUE) {
    log_lik <- logistic_loss(theta, x, y)
    if (verbose) {
        cat(paste(theta |> as.numeric(), collapse=", "), ": ", as.numeric(log_lik), "\n")
    }
    return(log_lik)
}
```

et vérifier qu'elle fonctionne

```{r}
eval_loss(torch_tensor(theta))
```

On utilise maintenant la magie de `torch` pour calculer le gradient de `eval_loss()` automatiquement.

```{r}
eval_loss_grad <- function(theta) {
    theta_grad <- torch_tensor(theta, requires_grad=TRUE)
    loss <- logistic_loss(theta = theta_grad, x, y)
    grad <- autograd_grad(loss, theta_grad)[[1]]
    return(grad)
}
```

On vérifie que la fonction fonctionne correctement:

```{r}
eval_loss_grad(theta)
```

```{r}
theta_grad <- torch_tensor(rep(0, length(theta)), requires_grad=TRUE)
optimizer <- optim_adam(theta_grad)
optimizer$zero_grad()
eval_loss(theta_grad, verbose = FALSE)$backward()
theta_grad$grad
optimizer$step()
```

## Exemple régression multivariée

Cet exemple a été réalisé à partir du blog [torch for optimization](https://blogs.rstudio.com/ai/posts/2021-04-22-torch-for-optimization/). Ici l'objectif est de réaliser l'optimisation d'un modèle linéaire (variance inclue) en 


```{r}
rm(list=ls())
library(tidyverse)
library(torch)


## Generate the data
X <- cbind(rep(1,100),matrix(rnorm(1000),100,10))
Beta.true <- rnorm(11)
Y <- X%*%Beta.true + rnorm(100)
n <- nrow(X)

## Declare the loss function
Theta <- torch_tensor(rep(1,12) %>% as.matrix, requires_grad = TRUE)
X.tensor <- torch_tensor(X)
LogLik <- function(theta){
  n*torch_log(theta[12]) + torch_square(torch_norm(Y-torch_matmul(X.tensor,theta[1:11])))/(2*torch_square(theta[12]))
}
LogLik(Theta)

## Check with an R equivalent
LogLik.r <- function(theta){
  n*log(theta[12]) + sum((Y-X%*%theta[1:11])**2)/(2*theta[12])
}
LogLik.r(as.matrix(rep(1,12)))


## Specify the optimization parameters
num_iterations <- 1000
lr <- 0.01
optimizer <- optim_adam(Theta,lr)
optimizer <- optim_rprop(Theta,lr)

## Optimization step description
calc_loss <- function() {
  
  optimizer$zero_grad()
  value <- LogLik(Theta)
  value$backward()
  value
}

## Run the optimization
LogLik.hist <- rep(NA,num_iterations)
for (i in 1:num_iterations) {
  LogLik.hist[i] <- optimizer$step(calc_loss) %>% as.numeric
}

## How does the loss function behave ?
plot(LogLik.hist,cex=0.4)

## Are the gradients at 0 ?
Theta$grad

## Compare the coef estimates with the ones of lm 
Res <- lm(Y ~ X[,-1] )
plot(Res$coefficients,as.numeric(Theta)[-12])
abline(a=0,b=1,col=2)

## Compare the variances
summary(Res)$sigma**2
Theta[12]
```

# Section Felix

## Fonctions de torch compatibles avec autograd

- `torch_ones(NB_OF_ROWS, NB_OF_COLS, requires_grad = TRUE)` ou `torch_ones(VECTOR_OF_DIMS, requires_grad = TRUE)` : crée un tenseur rempli de 1.
- `torch_tensor(OBJECT, requires_grad = TRUE)` : convertit un objet R (vecteur, matrice ou array) en un tenseur torch.
- `$mean()`, `$sum()`, `$pow(ORDER)`, `$mm(MATRIX)` (matrix multiplication), cf les fonctions torch_* de la documentation R de torch.
- Les fonctions de torch peuvent s'utiliser de manière analogue aux pipes. Par exemple, `x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)`.

- `output$backward()` : effectue la propagation backward pour calculer les gradients successifs.
- `input$grad` : récupère le gradient de `output` par rapport à `input`.
- `midput$retain_grad()` : à lancer avant `$backward()$ et permet de stocker les gradients intermédiaires.

## Comment implémenter une fonction compatible avec autograd

Une fonction compatible avec autograd doit être définie via `autograd_function`, et posséder deux méthodes, `forward` et `backward`, qui déterminent respectivement quelle opération est exécutée par le code et comment calculer le gradient.

- `ctx` correspond aux objets partagés entre la méthode `forward` et la méthode `backward`.
- `$save_for_backward()` permet de sauvegarder des valeurs des inputs et/ou outputs à utiliser lors du calcul du gradient dans la méthode `backward`.

Exemple de code :

```{r}
log_base = autograd_function(
    
    forward = function(ctx, input, base) {
        ctx$save_for_backward(input = input, base = base)
        input$log() / log(base)
    },
    
    backward = function(ctx, grad_output) {
        vars = ctx$saved_variables
        list(input = grad_output / (vars$input * log(vars$base)))
    }
    
)

x <- torch_tensor(2, requires_grad = TRUE)
y <- log_base(x, exp(1))
y$backward()
x$grad
```

`backward` doit être capable de propager le calcul du gradient aux inputs de la fonction implémentée.
Par exemple, en considérant que la fonction implémentée correspond à la fonction $f$ dans le chainage 
$$x \xrightarrow[]{f} y \rightarrow z,$$
la méthode `backward` doit sortir $\partial z / \partial x = (\partial y / \partial x)(\partial z / \partial y)$, où dans l'exemple précédent $\partial z / \partial y$ est représenté par l'argument `grad_output`.

